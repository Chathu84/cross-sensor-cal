{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c501955a-4e56-40df-93e4-346c6e5ad935",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spectral_unmixing_tools as el_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5c38436-e4bf-4a83-82ed-fdb1ec79276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/earthlab/cross-sensor-cal.git\n",
      "  Cloning https://github.com/earthlab/cross-sensor-cal.git to /tmp/pip-req-build-3mcb9iw1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/earthlab/cross-sensor-cal.git /tmp/pip-req-build-3mcb9iw1\n",
      "  Resolved https://github.com/earthlab/cross-sensor-cal.git to commit 8717d6b99deb35c43dc57fc2bec4882c2f254814\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.26.3)\n",
      "Requirement already satisfied: spectral>=0.22 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.23.1)\n",
      "Requirement already satisfied: geopandas>=0.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.14.2)\n",
      "Requirement already satisfied: rasterio>=1.1.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.3.8)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.2.2)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.9.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.31.0)\n",
      "Requirement already satisfied: ray>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.8.0)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (1.9.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (23.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (3.6.0)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: affine in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (8.1.3)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (0.7.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.4.7)\n",
      "Requirement already satisfied: click-plugins in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (69.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.0.5)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.23.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (6.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (2.0.7)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.11.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas>=0.8.0->EarthLabSpectral==0.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.17.1)\n",
      "Building wheels for collected packages: EarthLabSpectral\n",
      "  Building wheel for EarthLabSpectral (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EarthLabSpectral: filename=EarthLabSpectral-0.1-py3-none-any.whl size=1417 sha256=bebfc882dd13f60e077c2b288b2ae7dd343eae9db8bd57580f13cd335263fe64\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e_8ou5jp/wheels/d4/97/e8/c93ae5a8364c661ba5327b524ebe4ee01e41cfe582ac66f3d0\n",
      "Successfully built EarthLabSpectral\n",
      "Installing collected packages: EarthLabSpectral\n",
      "Successfully installed EarthLabSpectral-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/earthlab/cross-sensor-cal.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36314881-2c4a-43d0-b03c-e82595b2de0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance\n",
      "Command executed successfully for sensor type: Landsat 5 TM\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10782', 'samples': '1071', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 485.,  560.,  660.,  830., 1650., 2215.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 7 ETM+\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10782', 'samples': '1071', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 477.,  560.,  662.,  835., 1648., 2206.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 8 OLI\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10782', 'samples': '1071', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 480.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 9 OLI-2\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10782', 'samples': '1071', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 453.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200731_155024_reflectance\n",
      "Command executed successfully for sensor type: Landsat 5 TM\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '11138', 'samples': '1031', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 485.,  560.,  660.,  830., 1650., 2215.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 7 ETM+\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '11138', 'samples': '1031', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 477.,  560.,  662.,  835., 1648., 2206.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 8 OLI\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '11138', 'samples': '1031', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 480.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 9 OLI-2\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '11138', 'samples': '1031', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 453.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_155314_reflectance\n",
      "Command executed successfully for sensor type: Landsat 5 TM\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10785', 'samples': '1083', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 485.,  560.,  660.,  830., 1650., 2215.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 7 ETM+\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10785', 'samples': '1083', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 477.,  560.,  662.,  835., 1648., 2206.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 8 OLI\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10785', 'samples': '1083', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 480.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 9 OLI-2\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10785', 'samples': '1083', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 453.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_163444_reflectance\n",
      "Command executed successfully for sensor type: Landsat 5 TM\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10582', 'samples': '1110', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 485.,  560.,  660.,  830., 1650., 2215.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 7 ETM+\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10582', 'samples': '1110', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 477.,  560.,  662.,  835., 1648., 2206.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 8 OLI\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10582', 'samples': '1110', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 480.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 9 OLI-2\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10582', 'samples': '1110', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 453.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_170802_reflectance\n",
      "Command executed successfully for sensor type: Landsat 5 TM\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10487', 'samples': '994', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 485.,  560.,  660.,  830., 1650., 2215.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 7 ETM+\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10487', 'samples': '994', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 477.,  560.,  662.,  835., 1648., 2206.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 8 OLI\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10487', 'samples': '994', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 480.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "Command executed successfully for sensor type: Landsat 9 OLI-2\n",
      "Standard Output: {'description': 'Resampled hyperspectral data', 'file type': 'ENVI Standard', 'sensor type': 'landsat', 'bands': 6, 'lines': '10487', 'samples': '994', 'header offset': 0, 'data type': 4, 'interleave': 'bil', 'byte order': 0, 'wavelength units': 'Nanometers', 'wavelength': array([ 453.,  560.,  655.,  865., 1610., 2200.], dtype=float32)}\n",
      "\n",
      "done resampling\n"
     ]
    }
   ],
   "source": [
    "base_folder = \"NIWOT_calibration_flight_08_2020\"\n",
    "el_spectral.resample_translation_to_other_sensors(base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c5d65-796e-43a7-b464-6d3007d7210c",
   "metadata": {},
   "source": [
    "## Extract spectra as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe8b0c5-0171-4140-a9d2-e038fa3239d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None  # This will hold the raster data array\n",
    "        self.file_type = \"envi\"\n",
    "        # Other attributes...\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the raster data from the file_path into self.data\"\"\"\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            self.data = src.read()  # Read all bands\n",
    "\n",
    "    def get_chunk_from_extent(self, corrections=[], resample=False):\n",
    "\n",
    "        self.load_data()  # Ensure data is loaded\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            bounds = src.bounds\n",
    "            width, height = src.width, src.height\n",
    "            col_start, line_start = 0, 0\n",
    "            col_end, line_end = width, height\n",
    "\n",
    "            # Assuming self.data is a 3D numpy array with dimensions [bands, rows, cols]\n",
    "            chunk = self.data[:, line_start:line_end, col_start:col_end]\n",
    "            \n",
    "            # Apply any processing to chunk here...\n",
    "            # For example, to demonstrate, flip chunk vertically\n",
    "            chunk = np.flip(chunk, axis=1)\n",
    "            \n",
    "            return chunk\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_envi_file_path' with the actual path to your ENVI file\n",
    "raster_path_5 = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\"\n",
    "raster_path_7 = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\"\n",
    "raster_path_8 = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\"\n",
    "raster_path_9 = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\"\n",
    "raster_path_corrected = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\"\n",
    "raster_path_original = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectance\"\n",
    "\n",
    "\n",
    "\n",
    "processor = ENVIProcessor(raster_path_5)\n",
    "chunk_5 = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "processor = ENVIProcessor(raster_path_7)\n",
    "chunk_7 = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "processor = ENVIProcessor(raster_path_8)\n",
    "chunk_8 = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "processor = ENVIProcessor(raster_path_9)\n",
    "chunk_9 = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "\n",
    "processor = ENVIProcessor(raster_path_corrected)\n",
    "chunk_corrected = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "processor = ENVIProcessor(raster_path_original)\n",
    "chunk_original = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "\n",
    "#combined_array = np.concatenate((chunk_corrected, chunk_5, chunk_7, chunk_8, chunk_9), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9fccf1-031d-47a8-99dd-c22b4f88ddf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 10782, 1071)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_array = np.concatenate((chunk_corrected, chunk_original, chunk_5, chunk_7, chunk_8, chunk_9), axis=0)\n",
    "combined_array.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58be238-60a4-43d1-ad30-a893a3c09211",
   "metadata": {},
   "source": [
    "## flatten array into a 2D df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1039151f-97ae-48b2-b757-efa640080906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def flatten_array_to_dataframe(array):\n",
    "    \"\"\"\n",
    "    Flattens a 3D numpy array into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    \n",
    "    Returns:\n",
    "    - A pandas DataFrame where each row represents a pixel across all bands.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a 3D numpy array\n",
    "    if len(array.shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3-dimensional.\")\n",
    "    \n",
    "    bands, rows, cols = array.shape\n",
    "    # Reshape the array to have pixels as rows and bands as columns\n",
    "    reshaped_array = array.reshape(bands, -1).T  # Transpose to make bands as columns\n",
    "    \n",
    "    # Create a DataFrame from the reshaped array\n",
    "    df = pd.DataFrame(reshaped_array, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    \n",
    "    # Optionally, add pixel row and column indices\n",
    "    pixel_indices = np.indices((rows, cols)).reshape(2, -1).T  # 2D array of row,col indices for each pixel\n",
    "    df['Pixel_Row'] = pixel_indices[:, 0]\n",
    "    df['Pixel_Col'] = pixel_indices[:, 1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'chunk' is your 3D numpy array of shape (426, 11138, 1031)\n",
    "\n",
    "#df_5 = flatten_array_to_dataframe(chunk_5)\n",
    "df_corrected = flatten_array_to_dataframe(combined_array)\n",
    "df_corrected\n",
    "columns_order = ['Pixel_Row', 'Pixel_Col'] + [col for col in df_corrected.columns if col not in ['Pixel_Row', 'Pixel_Col']]\n",
    "df_corrected = df_corrected[columns_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f55111-3cb3-4b5d-a3bd-d7cf588ce1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming completed. The DataFrame has been updated in place.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_corrected is your input DataFrame\n",
    "\n",
    "# Calculate the expected new names count (for \"Band_\" columns).\n",
    "expected_new_names_count = 426 * 2 + 4 * 6  # Original + Corrected + 4 Landsat versions * 6\n",
    "\n",
    "# Verify the number of Band_ columns matches the expected count for renaming.\n",
    "if len([col for col in df_corrected.columns if col.startswith('Band_')]) == expected_new_names_count:\n",
    "    # Generate new column names.\n",
    "    new_column_names = []\n",
    "    new_column_names.extend([f\"Original_band_{i}\" for i in range(1, 427)])\n",
    "    new_column_names.extend([f\"Corrected_band_{i}\" for i in range(1, 427)])\n",
    "    for landsat_version in [5, 7, 8, 9]:  # Skipping Landsat 6 as per your note\n",
    "        new_column_names.extend([f\"Landsat_{landsat_version}_band_{i}\" for i in range(1, 7)])\n",
    "    \n",
    "    # Identify \"Band_\" columns.\n",
    "    band_columns = [col for col in df_corrected.columns if col.startswith('Band_')]\n",
    "    \n",
    "    # Create a mapping from old 'Band_' columns to new names.\n",
    "    # Instead of creating a new DataFrame, rename columns in place.\n",
    "    for old_name, new_name in zip(band_columns, new_column_names):\n",
    "        df_corrected.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "    print(\"Renaming completed. The DataFrame has been updated in place.\")\n",
    "\n",
    "    # Since the renaming is done in place, we can simply assign df_corrected to spectra_per_pixel.\n",
    "    spectra_per_pixel = df_corrected\n",
    "else:\n",
    "    print(\"Mismatch in the total expected 'Band_' column count. Please verify the DataFrame structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583055f8-cfe6-4f13-9859-c56ba24b7334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_id</th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>Original_band_1</th>\n",
       "      <th>Original_band_2</th>\n",
       "      <th>Original_band_3</th>\n",
       "      <th>Original_band_4</th>\n",
       "      <th>Original_band_5</th>\n",
       "      <th>Original_band_6</th>\n",
       "      <th>Original_band_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Landsat_8_band_3</th>\n",
       "      <th>Landsat_8_band_4</th>\n",
       "      <th>Landsat_8_band_5</th>\n",
       "      <th>Landsat_8_band_6</th>\n",
       "      <th>Landsat_9_band_1</th>\n",
       "      <th>Landsat_9_band_2</th>\n",
       "      <th>Landsat_9_band_3</th>\n",
       "      <th>Landsat_9_band_4</th>\n",
       "      <th>Landsat_9_band_5</th>\n",
       "      <th>Landsat_9_band_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547517</th>\n",
       "      <td>11547517</td>\n",
       "      <td>10781</td>\n",
       "      <td>1066</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547518</th>\n",
       "      <td>11547518</td>\n",
       "      <td>10781</td>\n",
       "      <td>1067</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547519</th>\n",
       "      <td>11547519</td>\n",
       "      <td>10781</td>\n",
       "      <td>1068</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547520</th>\n",
       "      <td>11547520</td>\n",
       "      <td>10781</td>\n",
       "      <td>1069</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547521</th>\n",
       "      <td>11547521</td>\n",
       "      <td>10781</td>\n",
       "      <td>1070</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11547522 rows × 879 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pixel_id  Pixel_Row  Pixel_Col  Original_band_1  Original_band_2  \\\n",
       "0                0          0          0          -9999.0          -9999.0   \n",
       "1                1          0          1          -9999.0          -9999.0   \n",
       "2                2          0          2          -9999.0          -9999.0   \n",
       "3                3          0          3          -9999.0          -9999.0   \n",
       "4                4          0          4          -9999.0          -9999.0   \n",
       "...            ...        ...        ...              ...              ...   \n",
       "11547517  11547517      10781       1066          -9999.0          -9999.0   \n",
       "11547518  11547518      10781       1067          -9999.0          -9999.0   \n",
       "11547519  11547519      10781       1068          -9999.0          -9999.0   \n",
       "11547520  11547520      10781       1069          -9999.0          -9999.0   \n",
       "11547521  11547521      10781       1070          -9999.0          -9999.0   \n",
       "\n",
       "          Original_band_3  Original_band_4  Original_band_5  Original_band_6  \\\n",
       "0                 -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "1                 -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "2                 -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "3                 -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "4                 -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "...                   ...              ...              ...              ...   \n",
       "11547517          -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "11547518          -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "11547519          -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "11547520          -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "11547521          -9999.0          -9999.0          -9999.0          -9999.0   \n",
       "\n",
       "          Original_band_7  ...  Landsat_8_band_3  Landsat_8_band_4  \\\n",
       "0                 -9999.0  ...           -9999.0           -9999.0   \n",
       "1                 -9999.0  ...           -9999.0           -9999.0   \n",
       "2                 -9999.0  ...           -9999.0           -9999.0   \n",
       "3                 -9999.0  ...           -9999.0           -9999.0   \n",
       "4                 -9999.0  ...           -9999.0           -9999.0   \n",
       "...                   ...  ...               ...               ...   \n",
       "11547517          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547518          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547519          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547520          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547521          -9999.0  ...           -9999.0           -9999.0   \n",
       "\n",
       "          Landsat_8_band_5  Landsat_8_band_6  Landsat_9_band_1  \\\n",
       "0                  -9999.0      -9998.857661      -9998.999998   \n",
       "1                  -9999.0      -9998.857661      -9998.999998   \n",
       "2                  -9999.0      -9998.857661      -9998.999998   \n",
       "3                  -9999.0      -9998.857661      -9998.999998   \n",
       "4                  -9999.0      -9998.857661      -9998.999998   \n",
       "...                    ...               ...               ...   \n",
       "11547517           -9999.0      -9998.857661      -9998.999998   \n",
       "11547518           -9999.0      -9998.857661      -9998.999998   \n",
       "11547519           -9999.0      -9998.857661      -9998.999998   \n",
       "11547520           -9999.0      -9998.857661      -9998.999998   \n",
       "11547521           -9999.0      -9998.857661      -9998.999998   \n",
       "\n",
       "          Landsat_9_band_2  Landsat_9_band_3  Landsat_9_band_4  \\\n",
       "0                  -9999.0           -9999.0           -9999.0   \n",
       "1                  -9999.0           -9999.0           -9999.0   \n",
       "2                  -9999.0           -9999.0           -9999.0   \n",
       "3                  -9999.0           -9999.0           -9999.0   \n",
       "4                  -9999.0           -9999.0           -9999.0   \n",
       "...                    ...               ...               ...   \n",
       "11547517           -9999.0           -9999.0           -9999.0   \n",
       "11547518           -9999.0           -9999.0           -9999.0   \n",
       "11547519           -9999.0           -9999.0           -9999.0   \n",
       "11547520           -9999.0           -9999.0           -9999.0   \n",
       "11547521           -9999.0           -9999.0           -9999.0   \n",
       "\n",
       "          Landsat_9_band_5  Landsat_9_band_6  \n",
       "0                  -9999.0      -9998.857661  \n",
       "1                  -9999.0      -9998.857661  \n",
       "2                  -9999.0      -9998.857661  \n",
       "3                  -9999.0      -9998.857661  \n",
       "4                  -9999.0      -9998.857661  \n",
       "...                    ...               ...  \n",
       "11547517           -9999.0      -9998.857661  \n",
       "11547518           -9999.0      -9998.857661  \n",
       "11547519           -9999.0      -9998.857661  \n",
       "11547520           -9999.0      -9998.857661  \n",
       "11547521           -9999.0      -9998.857661  \n",
       "\n",
       "[11547522 rows x 879 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra_per_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ddd21c-fb1e-41c4-a4a1-6605021fd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'spectra_per_pixel' is your DataFrame\n",
    "\n",
    "# Step 1: Create the 'Pixel_id' column by copying the index\n",
    "# This step is essentially just preparation, no heavy memory operation is involved here\n",
    "pixel_id_series = spectra_per_pixel.index.to_series()\n",
    "\n",
    "# Step 2: Insert the 'Pixel_id' column at position 0\n",
    "# The .insert() method is more memory efficient for this operation\n",
    "spectra_per_pixel.insert(0, 'Pixel_id', pixel_id_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56916c3a-60fe-4309-bffc-f8712256d5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning process, total rows: 11547522, chunk size: 100000, total chunks: 116\n",
      "Processed and wrote chunk 1/116 to CSV.\n",
      "Processed and wrote chunk 2/116 to CSV.\n",
      "Processed and wrote chunk 3/116 to CSV.\n",
      "Processed and wrote chunk 4/116 to CSV.\n",
      "Processed and wrote chunk 5/116 to CSV.\n",
      "Processed and wrote chunk 6/116 to CSV.\n",
      "Processed and wrote chunk 7/116 to CSV.\n",
      "Processed and wrote chunk 8/116 to CSV.\n",
      "Processed and wrote chunk 9/116 to CSV.\n",
      "Processed and wrote chunk 10/116 to CSV.\n",
      "Processed and wrote chunk 11/116 to CSV.\n",
      "Processed and wrote chunk 12/116 to CSV.\n",
      "Processed and wrote chunk 13/116 to CSV.\n",
      "Processed and wrote chunk 14/116 to CSV.\n",
      "Processed and wrote chunk 15/116 to CSV.\n",
      "Processed and wrote chunk 16/116 to CSV.\n",
      "Processed and wrote chunk 17/116 to CSV.\n",
      "Processed and wrote chunk 18/116 to CSV.\n",
      "Processed and wrote chunk 19/116 to CSV.\n",
      "Processed and wrote chunk 20/116 to CSV.\n",
      "Processed and wrote chunk 21/116 to CSV.\n",
      "Processed and wrote chunk 22/116 to CSV.\n",
      "Processed and wrote chunk 23/116 to CSV.\n",
      "Processed and wrote chunk 24/116 to CSV.\n",
      "Processed and wrote chunk 25/116 to CSV.\n",
      "Processed and wrote chunk 26/116 to CSV.\n",
      "Processed and wrote chunk 27/116 to CSV.\n",
      "Processed and wrote chunk 28/116 to CSV.\n",
      "Processed and wrote chunk 29/116 to CSV.\n",
      "Processed and wrote chunk 30/116 to CSV.\n",
      "Processed and wrote chunk 31/116 to CSV.\n",
      "Processed and wrote chunk 32/116 to CSV.\n",
      "Processed and wrote chunk 33/116 to CSV.\n",
      "Processed and wrote chunk 34/116 to CSV.\n",
      "Processed and wrote chunk 35/116 to CSV.\n",
      "Processed and wrote chunk 36/116 to CSV.\n",
      "Processed and wrote chunk 37/116 to CSV.\n",
      "Processed and wrote chunk 38/116 to CSV.\n",
      "Processed and wrote chunk 39/116 to CSV.\n",
      "Processed and wrote chunk 40/116 to CSV.\n",
      "Processed and wrote chunk 41/116 to CSV.\n",
      "Processed and wrote chunk 42/116 to CSV.\n",
      "Processed and wrote chunk 43/116 to CSV.\n",
      "Processed and wrote chunk 44/116 to CSV.\n",
      "Processed and wrote chunk 45/116 to CSV.\n",
      "Processed and wrote chunk 46/116 to CSV.\n",
      "Processed and wrote chunk 47/116 to CSV.\n",
      "Processed and wrote chunk 48/116 to CSV.\n",
      "Processed and wrote chunk 49/116 to CSV.\n",
      "Processed and wrote chunk 50/116 to CSV.\n",
      "Processed and wrote chunk 51/116 to CSV.\n",
      "Processed and wrote chunk 52/116 to CSV.\n",
      "Processed and wrote chunk 53/116 to CSV.\n",
      "Processed and wrote chunk 54/116 to CSV.\n",
      "Processed and wrote chunk 55/116 to CSV.\n",
      "Processed and wrote chunk 56/116 to CSV.\n",
      "Processed and wrote chunk 57/116 to CSV.\n",
      "Processed and wrote chunk 58/116 to CSV.\n",
      "Processed and wrote chunk 59/116 to CSV.\n",
      "Processed and wrote chunk 60/116 to CSV.\n",
      "Processed and wrote chunk 61/116 to CSV.\n",
      "Processed and wrote chunk 62/116 to CSV.\n",
      "Processed and wrote chunk 63/116 to CSV.\n",
      "Processed and wrote chunk 64/116 to CSV.\n",
      "Processed and wrote chunk 65/116 to CSV.\n",
      "Processed and wrote chunk 66/116 to CSV.\n",
      "Processed and wrote chunk 67/116 to CSV.\n",
      "Processed and wrote chunk 68/116 to CSV.\n",
      "Processed and wrote chunk 69/116 to CSV.\n",
      "Processed and wrote chunk 70/116 to CSV.\n",
      "Processed and wrote chunk 71/116 to CSV.\n",
      "Processed and wrote chunk 72/116 to CSV.\n",
      "Processed and wrote chunk 73/116 to CSV.\n",
      "Processed and wrote chunk 74/116 to CSV.\n",
      "Processed and wrote chunk 75/116 to CSV.\n",
      "Processed and wrote chunk 76/116 to CSV.\n",
      "Processed and wrote chunk 77/116 to CSV.\n",
      "Processed and wrote chunk 78/116 to CSV.\n",
      "Processed and wrote chunk 79/116 to CSV.\n",
      "Processed and wrote chunk 80/116 to CSV.\n",
      "Processed and wrote chunk 81/116 to CSV.\n",
      "Processed and wrote chunk 82/116 to CSV.\n",
      "Processed and wrote chunk 83/116 to CSV.\n",
      "Processed and wrote chunk 84/116 to CSV.\n",
      "Processed and wrote chunk 85/116 to CSV.\n",
      "Processed and wrote chunk 86/116 to CSV.\n",
      "Processed and wrote chunk 87/116 to CSV.\n",
      "Processed and wrote chunk 88/116 to CSV.\n",
      "Processed and wrote chunk 89/116 to CSV.\n",
      "Processed and wrote chunk 90/116 to CSV.\n",
      "Processed and wrote chunk 91/116 to CSV.\n",
      "Processed and wrote chunk 92/116 to CSV.\n",
      "Processed and wrote chunk 93/116 to CSV.\n",
      "Processed and wrote chunk 94/116 to CSV.\n",
      "Processed and wrote chunk 95/116 to CSV.\n",
      "Processed and wrote chunk 96/116 to CSV.\n",
      "Processed and wrote chunk 97/116 to CSV.\n",
      "Processed and wrote chunk 98/116 to CSV.\n",
      "Processed and wrote chunk 99/116 to CSV.\n",
      "Processed and wrote chunk 100/116 to CSV.\n",
      "Processed and wrote chunk 101/116 to CSV.\n",
      "Processed and wrote chunk 102/116 to CSV.\n",
      "Processed and wrote chunk 103/116 to CSV.\n",
      "Processed and wrote chunk 104/116 to CSV.\n",
      "Processed and wrote chunk 105/116 to CSV.\n",
      "Processed and wrote chunk 106/116 to CSV.\n",
      "Processed and wrote chunk 107/116 to CSV.\n",
      "Processed and wrote chunk 108/116 to CSV.\n",
      "Processed and wrote chunk 109/116 to CSV.\n",
      "Processed and wrote chunk 110/116 to CSV.\n",
      "Processed and wrote chunk 111/116 to CSV.\n",
      "Processed and wrote chunk 112/116 to CSV.\n",
      "Processed and wrote chunk 113/116 to CSV.\n",
      "Processed and wrote chunk 114/116 to CSV.\n",
      "Processed and wrote chunk 115/116 to CSV.\n",
      "Processed and wrote chunk 116/116 to CSV.\n",
      "Cleaning process completed and data written to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_data_and_write_to_csv(df, output_csv_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame in chunks to minimize memory usage and writes the cleaned\n",
    "    chunks directly to a CSV file to avoid memory overload. It replaces values approximately\n",
    "    equal to -9999 (within a tolerance of 1) with NaN in columns not starting with 'Pixel', and\n",
    "    then drops rows where all such columns are NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to clean.\n",
    "    - output_csv_path: Path to the output CSV file.\n",
    "    - chunk_size: Number of rows in each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - None. The cleaned data is written directly to the specified CSV file.\n",
    "    \"\"\"\n",
    "    total_rows = df.shape[0]\n",
    "    num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size else 0)\n",
    "\n",
    "    print(f\"Starting cleaning process, total rows: {total_rows}, chunk size: {chunk_size}, total chunks: {num_chunks}\")\n",
    "\n",
    "    # Initialize CSV file writing\n",
    "    first_chunk = True\n",
    "\n",
    "    for i, start_row in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk = df.iloc[start_row:start_row + chunk_size].copy()\n",
    "\n",
    "        # Replace values close to -9999 with NaN\n",
    "        for col in chunk.columns:\n",
    "            if not col.startswith('Pixel'):\n",
    "                chunk[col] = np.where(np.isclose(chunk[col], -9999, atol=1), np.nan, chunk[col])\n",
    "\n",
    "        # Drop rows where all non-'Pixel' columns are NaN\n",
    "        non_pixel_columns = [col for col in chunk.columns if not col.startswith('Pixel')]\n",
    "        chunk.dropna(subset=non_pixel_columns, how='all', inplace=True)\n",
    "        \n",
    "        # Write processed chunk to CSV\n",
    "        if first_chunk:\n",
    "            chunk.to_csv(output_csv_path, mode='w', header=True, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_csv(output_csv_path, mode='a', header=False, index=False)\n",
    "        \n",
    "        print(f\"Processed and wrote chunk {i+1}/{num_chunks} to CSV.\")\n",
    "\n",
    "    print(\"Cleaning process completed and data written to CSV.\")\n",
    "\n",
    "# Specify the output CSV path\n",
    "output_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectanc_active_pixels.csv'\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "clean_data_and_write_to_csv(spectra_per_pixel, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037be62c-97e0-48c2-aa40-0791e0704b48",
   "metadata": {},
   "source": [
    "## Reshape data frame and change labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6ac2ce0-247d-4576-8521-cb53d0504289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 'Band_1', -9998.999997820287)\n",
      "(0, 1, 'Band_1', -9998.999997820287)\n",
      "(0, 2, 'Band_1', -9998.999997820287)\n",
      "(0, 3, 'Band_1', -9998.999997820287)\n",
      "(0, 4, 'Band_1', -9998.999997820287)\n",
      "(0, 5, 'Band_1', -9998.999997820287)\n",
      "(0, 6, 'Band_1', -9998.999997820287)\n",
      "(0, 7, 'Band_1', -9998.999997820287)\n",
      "(0, 8, 'Band_1', -9998.999997820287)\n",
      "(0, 9, 'Band_1', -9998.999997820287)\n",
      "(0, 10, 'Band_1', -9998.999997820287)\n",
      "(0, 11, 'Band_1', -9998.999997820287)\n",
      "(0, 12, 'Band_1', -9998.999997820287)\n",
      "(0, 13, 'Band_1', -9998.999997820287)\n",
      "(0, 14, 'Band_1', -9998.999997820287)\n",
      "(0, 15, 'Band_1', -9998.999997820287)\n",
      "(0, 16, 'Band_1', -9998.999997820287)\n",
      "(0, 17, 'Band_1', -9998.999997820287)\n",
      "(0, 18, 'Band_1', -9998.999997820287)\n",
      "(0, 19, 'Band_1', -9998.999997820287)\n",
      "(0, 20, 'Band_1', -9998.999997820287)\n",
      "(0, 21, 'Band_1', -9998.999997820287)\n",
      "(0, 22, 'Band_1', -9998.999997820287)\n",
      "(0, 23, 'Band_1', -9998.999997820287)\n",
      "(0, 24, 'Band_1', -9998.999997820287)\n",
      "(0, 25, 'Band_1', -9998.999997820287)\n",
      "(0, 26, 'Band_1', -9998.999997820287)\n",
      "(0, 27, 'Band_1', -9998.999997820287)\n",
      "(0, 28, 'Band_1', -9998.999997820287)\n",
      "(0, 29, 'Band_1', -9998.999997820287)\n",
      "(0, 30, 'Band_1', -9998.999997820287)\n",
      "(0, 31, 'Band_1', -9998.999997820287)\n",
      "(0, 32, 'Band_1', -9998.999997820287)\n",
      "(0, 33, 'Band_1', -9998.999997820287)\n",
      "(0, 34, 'Band_1', -9998.999997820287)\n",
      "(0, 35, 'Band_1', -9998.999997820287)\n",
      "(0, 36, 'Band_1', -9998.999997820287)\n",
      "(0, 37, 'Band_1', -9998.999997820287)\n",
      "(0, 38, 'Band_1', -9998.999997820287)\n",
      "(0, 39, 'Band_1', -9998.999997820287)\n",
      "(0, 40, 'Band_1', -9998.999997820287)\n",
      "(0, 41, 'Band_1', -9998.999997820287)\n",
      "(0, 42, 'Band_1', -9998.999997820287)\n",
      "(0, 43, 'Band_1', -9998.999997820287)\n",
      "(0, 44, 'Band_1', -9998.999997820287)\n",
      "(0, 45, 'Band_1', -9998.999997820287)\n",
      "(0, 46, 'Band_1', -9998.999997820287)\n",
      "(0, 47, 'Band_1', -9998.999997820287)\n",
      "(0, 48, 'Band_1', -9998.999997820287)\n",
      "(0, 49, 'Band_1', -9998.999997820287)\n",
      "(0, 50, 'Band_1', -9998.999997820287)\n",
      "(0, 51, 'Band_1', -9998.999997820287)\n",
      "(0, 52, 'Band_1', -9998.999997820287)\n",
      "(0, 53, 'Band_1', -9998.999997820287)\n",
      "(0, 54, 'Band_1', -9998.999997820287)\n",
      "(0, 55, 'Band_1', -9998.999997820287)\n",
      "(0, 56, 'Band_1', -9998.999997820287)\n",
      "(0, 57, 'Band_1', -9998.999997820287)\n",
      "(0, 58, 'Band_1', -9998.999997820287)\n",
      "(0, 59, 'Band_1', -9998.999997820287)\n",
      "(0, 60, 'Band_1', -9998.999997820287)\n",
      "(0, 61, 'Band_1', -9998.999997820287)\n",
      "(0, 62, 'Band_1', -9998.999997820287)\n",
      "(0, 63, 'Band_1', -9998.999997820287)\n",
      "(0, 64, 'Band_1', -9998.999997820287)\n",
      "(0, 65, 'Band_1', -9998.999997820287)\n",
      "(0, 66, 'Band_1', -9998.999997820287)\n",
      "(0, 67, 'Band_1', -9998.999997820287)\n",
      "(0, 68, 'Band_1', -9998.999997820287)\n",
      "(0, 69, 'Band_1', -9998.999997820287)\n",
      "(0, 70, 'Band_1', -9998.999997820287)\n",
      "(0, 71, 'Band_1', -9998.999997820287)\n",
      "(0, 72, 'Band_1', -9998.999997820287)\n",
      "(0, 73, 'Band_1', -9998.999997820287)\n",
      "(0, 74, 'Band_1', -9998.999997820287)\n",
      "(0, 75, 'Band_1', -9998.999997820287)\n",
      "(0, 76, 'Band_1', -9998.999997820287)\n",
      "(0, 77, 'Band_1', -9998.999997820287)\n",
      "(0, 78, 'Band_1', -9998.999997820287)\n",
      "(0, 79, 'Band_1', -9998.999997820287)\n",
      "(0, 80, 'Band_1', -9998.999997820287)\n",
      "(0, 81, 'Band_1', -9998.999997820287)\n",
      "(0, 82, 'Band_1', -9998.999997820287)\n",
      "(0, 83, 'Band_1', -9998.999997820287)\n",
      "(0, 84, 'Band_1', -9998.999997820287)\n",
      "(0, 85, 'Band_1', -9998.999997820287)\n",
      "(0, 86, 'Band_1', -9998.999997820287)\n",
      "(0, 87, 'Band_1', -9998.999997820287)\n",
      "(0, 88, 'Band_1', -9998.999997820287)\n",
      "(0, 89, 'Band_1', -9998.999997820287)\n",
      "(0, 90, 'Band_1', -9998.999997820287)\n",
      "(0, 91, 'Band_1', -9998.999997820287)\n",
      "(0, 92, 'Band_1', -9998.999997820287)\n",
      "(0, 93, 'Band_1', -9998.999997820287)\n",
      "(0, 94, 'Band_1', -9998.999997820287)\n",
      "(0, 95, 'Band_1', -9998.999997820287)\n",
      "(0, 96, 'Band_1', -9998.999997820287)\n",
      "(0, 97, 'Band_1', -9998.999997820287)\n",
      "(0, 98, 'Band_1', -9998.999997820287)\n",
      "(0, 99, 'Band_1', -9998.999997820287)\n",
      "(0, 100, 'Band_1', -9998.999997820287)\n",
      "(0, 101, 'Band_1', -9998.999997820287)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iterate_flatten_melt_array(array):\n",
    "    \"\"\"\n",
    "    Generator to iterate over a 3D numpy array and yield \"melted\" data.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    \n",
    "    Yields:\n",
    "    - Tuple of (Pixel_Row, Pixel_Col, Band_ID, Wavelength) for each pixel-band combination.\n",
    "    \"\"\"\n",
    "    bands, rows, cols = array.shape\n",
    "    \n",
    "    for band in range(bands):\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                yield (row, col, f'Band_{band+1}', array[band, row, col])\n",
    "\n",
    "# Example usage\n",
    "#chunk = np.random.rand(426, 11138, 1031)  # Replace with your actual data\n",
    "\n",
    "# To demonstrate or test the generator, you can iterate through a small portion of it\n",
    "for i, data_point in enumerate(iterate_flatten_melt_array(chunk)):\n",
    "    print(data_point)\n",
    "    if i > 100:  # Adjust this condition to control how many items you want to print\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3768125-a13d-4c7e-8dfd-6cad4d1f0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('melted_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pixel_Row', 'Pixel_Col', 'Band_ID', 'Wavelength'])  # Write header\n",
    "\n",
    "    # Write each data point\n",
    "    for data_point in iterate_flatten_melt_array(chunk):\n",
    "        writer.writerow(data_point)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def batch_flatten_melt_array(array, batch_size=1000000):\n",
    "    \"\"\"\n",
    "    Generator to iterate over a 3D numpy array and yield batches of \"melted\" data.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    - batch_size: The number of rows in each batch.\n",
    "    \n",
    "    Yields:\n",
    "    - A DataFrame containing a batch of melted data.\n",
    "    \"\"\"\n",
    "    bands, rows, cols = array.shape\n",
    "    total_pixels = rows * cols\n",
    "    num_batches = (total_pixels + batch_size - 1) // batch_size  # Ceiling division to get the number of batches\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        batch_data = []\n",
    "        start_index = batch * batch_size\n",
    "        end_index = min(start_index + batch_size, total_pixels)\n",
    "        \n",
    "        for index in range(start_index, end_index):\n",
    "            row = index // cols\n",
    "            col = index % cols\n",
    "            for band in range(bands):\n",
    "                batch_data.append((row, col, f'Band_{band+1}', array[band, row, col]))\n",
    "                \n",
    "        batch_df = pd.DataFrame(batch_data, columns=['Pixel_Row', 'Pixel_Col', 'Band_ID', 'Wavelength'])\n",
    "        yield batch_df\n",
    "\n",
    "# Example usage\n",
    "#chunk = np.random.rand(426, 11138, 1031)  # Replace with your actual data\n",
    "\n",
    "# Iterate through each batch and process\n",
    "for i, batch_df in enumerate(batch_flatten_melt_array(chunk)):\n",
    "    print(f\"Processing batch {i+1}\")\n",
    "    # Process the batch_df here\n",
    "    # For example, you could save each batch to a separate CSV file\n",
    "    batch_df.to_csv(f'melted_data_batch_{i+1}.csv', index=False)\n",
    "   # if i == 0:  # For demonstration, break after processing the first batch\n",
    "    #    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6d650-7c31-4100-bd3e-4a06a0af777d",
   "metadata": {},
   "source": [
    "## Exract by polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551e6f3c-68ea-446a-806d-d789ab16d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>EditDate</th>\n",
       "      <th>Editor</th>\n",
       "      <th>description_notes</th>\n",
       "      <th>dbh</th>\n",
       "      <th>tree_height</th>\n",
       "      <th>...</th>\n",
       "      <th>og_flight_date</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>collector_name</th>\n",
       "      <th>plot</th>\n",
       "      <th>location</th>\n",
       "      <th>woody_shrub_height</th>\n",
       "      <th>imagery</th>\n",
       "      <th>combined_all_category_species</th>\n",
       "      <th>area_m</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1223</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1224</td>\n",
       "      <td>558</td>\n",
       "      <td>{54D1A59F-2B5C-4D76-913A-690314E314A7}</td>\n",
       "      <td>2023-06-20 19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Non-vegetated &amp; dead_Rock__</td>\n",
       "      <td>6.064980</td>\n",
       "      <td>POLYGON ((452524.7605621438 4435014.4842204545...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>808</td>\n",
       "      <td>548</td>\n",
       "      <td>{C3209290-3F79-4268-AE0F-4BE7F43E1C71}</td>\n",
       "      <td>2023-06-20 21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:03:43+00:00</td>\n",
       "      <td>2023-06-20 21:03:34+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>23.032479</td>\n",
       "      <td>POLYGON ((452510.4188020002 4435431.162891659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>792</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>792</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>793</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>793</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pixel_Row  Pixel_Col                                GlobalID  \\\n",
       "0         1222        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "1         1222        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "2         1223        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "3         1223        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "4         1224        558  {54D1A59F-2B5C-4D76-913A-690314E314A7}   \n",
       "..         ...        ...                                     ...   \n",
       "492        808        548  {C3209290-3F79-4268-AE0F-4BE7F43E1C71}   \n",
       "493        792        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "494        792        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "495        793        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "496        793        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "\n",
       "                 CreationDate                     Creator  \\\n",
       "0   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "1   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "2   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "4   2023-06-20 19:28:37+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "..                        ...                         ...   \n",
       "492 2023-06-20 21:04:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "493 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "494 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "495 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "496 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "\n",
       "                      EditDate                      Editor description_notes  \\\n",
       "0    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "1    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "2    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "4    2023-06-20T19:28:37+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "..                         ...                         ...               ...   \n",
       "492  2023-06-20T21:04:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "493  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "494  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "495  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "496  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "\n",
       "      dbh tree_height  ...            og_flight_date  \\\n",
       "0    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "1    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "2    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "3    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "4    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "..    ...         ...  ...                       ...   \n",
       "492  None        None  ... 2023-06-20 21:03:43+00:00   \n",
       "493  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "494  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "495  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "496  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "\n",
       "              collection_date collector_name plot  location  \\\n",
       "0   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "1   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "2   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "3   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "4   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "..                        ...            ...  ...       ...   \n",
       "492 2023-06-20 21:03:34+00:00          Katie    0  Brainard   \n",
       "493 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "494 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "495 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "496 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "\n",
       "    woody_shrub_height imagery  \\\n",
       "0                 None     AOP   \n",
       "1                 None     AOP   \n",
       "2                 None     AOP   \n",
       "3                 None     AOP   \n",
       "4                 None     AOP   \n",
       "..                 ...     ...   \n",
       "492         10cm to 1m     AOP   \n",
       "493         10cm to 1m     AOP   \n",
       "494         10cm to 1m     AOP   \n",
       "495         10cm to 1m     AOP   \n",
       "496         10cm to 1m     AOP   \n",
       "\n",
       "                         combined_all_category_species     area_m  \\\n",
       "0                        Evergreen___Picea engelmannii   7.004149   \n",
       "1                        Evergreen___Picea engelmannii   7.004149   \n",
       "2                        Evergreen___Picea engelmannii   7.004149   \n",
       "3                        Evergreen___Picea engelmannii   7.004149   \n",
       "4                          Non-vegetated & dead_Rock__   6.064980   \n",
       "..                                                 ...        ...   \n",
       "492  Woody shrub_Woody shrub - Broadleaf__Unidentified  23.032479   \n",
       "493  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "494  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "495  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "496  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "1    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "2    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "3    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "4    POLYGON ((452524.7605621438 4435014.4842204545...  \n",
       "..                                                 ...  \n",
       "492  POLYGON ((452510.4188020002 4435431.162891659,...  \n",
       "493  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "494  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "495  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "496  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "\n",
       "[497 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, raster_path, polygons_path):\n",
    "        self.raster_path = raster_path\n",
    "        self.polygons_path = polygons_path\n",
    "        self.polygons = None\n",
    "        self.raster_meta = None\n",
    "        \n",
    "    def load_polygons(self):\n",
    "        \"\"\"Loads the polygons and ensures they are in the same CRS as the raster.\"\"\"\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            self.raster_meta = src.meta\n",
    "            self.polygons = gpd.read_file(self.polygons_path)\n",
    "            self.polygons = self.polygons.to_crs(src.crs)\n",
    "    \n",
    "    def extract_data_by_polygons(self):\n",
    "        \"\"\"Extracts the row and col indices from the raster for each polygon and appends all attributes from the polygons.\"\"\"\n",
    "        self.load_polygons()  # Load polygons and ensure CRS match\n",
    "        \n",
    "        all_data = []\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            raster_bounds = src.bounds\n",
    "            raster_box = box(*raster_bounds)\n",
    "            \n",
    "            for _, poly in self.polygons.iterrows():\n",
    "                geom = poly.geometry\n",
    "                # Skip invalid or empty geometries or those that do not intersect with the raster\n",
    "                if geom is None or geom.is_empty or not geom.intersects(raster_box):\n",
    "                    continue\n",
    "                \n",
    "                window = from_bounds(*geom.bounds, transform=src.transform)\n",
    "                # Skip windows that are completely outside the raster bounds\n",
    "                if window.width <= 0 or window.height <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert window offsets to integers\n",
    "                row_off = int(window.row_off)\n",
    "                col_off = int(window.col_off)\n",
    "                # Extract the rows and cols from the window\n",
    "                rows = range(row_off, row_off + int(window.height))\n",
    "                cols = range(col_off, col_off + int(window.width))\n",
    "                \n",
    "                # Collect all attributes from the polygon\n",
    "                attributes = poly.to_dict()\n",
    "                \n",
    "                # Append the rows and cols along with the polygon attributes to the data list\n",
    "                for row in rows:\n",
    "                    for col in cols:\n",
    "                        pixel_data = {\n",
    "                            'Pixel_Row': row,\n",
    "                            'Pixel_Col': col,\n",
    "                            **attributes  # This adds all polygon attributes\n",
    "                        }\n",
    "                        all_data.append(pixel_data)\n",
    "        \n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpkg_path = 'Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg'\n",
    "existing_raster_path = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\"\n",
    "processor = ENVIProcessor(existing_raster_path, gpkg_path)\n",
    "df_polygons = processor.extract_data_by_polygons()\n",
    "df_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb73915d-a626-4b9b-8578-b00551760870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_value_band_1</th>\n",
       "      <th>row</th>\n",
       "      <th>col</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>pixel_value_band_2</th>\n",
       "      <th>pixel_value_band_3</th>\n",
       "      <th>pixel_value_band_4</th>\n",
       "      <th>pixel_value_band_5</th>\n",
       "      <th>pixel_value_band_6</th>\n",
       "      <th>pixel_value_band_7</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_value_band_417</th>\n",
       "      <th>pixel_value_band_418</th>\n",
       "      <th>pixel_value_band_419</th>\n",
       "      <th>pixel_value_band_420</th>\n",
       "      <th>pixel_value_band_421</th>\n",
       "      <th>pixel_value_band_422</th>\n",
       "      <th>pixel_value_band_423</th>\n",
       "      <th>pixel_value_band_424</th>\n",
       "      <th>pixel_value_band_425</th>\n",
       "      <th>pixel_value_band_426</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>739.0</td>\n",
       "      <td>1222</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>1222</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1223</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>429.0</td>\n",
       "      <td>1223</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>383.0</td>\n",
       "      <td>1224</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294355</th>\n",
       "      <td>NaN</td>\n",
       "      <td>792</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>823.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294356</th>\n",
       "      <td>NaN</td>\n",
       "      <td>792</td>\n",
       "      <td>552</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294357</th>\n",
       "      <td>NaN</td>\n",
       "      <td>793</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>339.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294358</th>\n",
       "      <td>NaN</td>\n",
       "      <td>793</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>491.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294359</th>\n",
       "      <td>NaN</td>\n",
       "      <td>793</td>\n",
       "      <td>552</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293675 rows × 429 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pixel_value_band_1   row  col                                GlobalID  \\\n",
       "0                    739.0  1222  554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "1                    300.0  1222  555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "2                     26.0  1223  554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "3                    429.0  1223  555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "4                    383.0  1224  554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "...                    ...   ...  ...                                     ...   \n",
       "294355                 NaN   792  551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "294356                 NaN   792  552  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "294357                 NaN   793  550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "294358                 NaN   793  551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "294359                 NaN   793  552  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "\n",
       "        pixel_value_band_2  pixel_value_band_3  pixel_value_band_4  \\\n",
       "0                      NaN                 NaN                 NaN   \n",
       "1                      NaN                 NaN                 NaN   \n",
       "2                      NaN                 NaN                 NaN   \n",
       "3                      NaN                 NaN                 NaN   \n",
       "4                      NaN                 NaN                 NaN   \n",
       "...                    ...                 ...                 ...   \n",
       "294355                 NaN                 NaN                 NaN   \n",
       "294356                 NaN                 NaN                 NaN   \n",
       "294357                 NaN                 NaN                 NaN   \n",
       "294358                 NaN                 NaN                 NaN   \n",
       "294359                 NaN                 NaN                 NaN   \n",
       "\n",
       "        pixel_value_band_5  pixel_value_band_6  pixel_value_band_7  ...  \\\n",
       "0                      NaN                 NaN                 NaN  ...   \n",
       "1                      NaN                 NaN                 NaN  ...   \n",
       "2                      NaN                 NaN                 NaN  ...   \n",
       "3                      NaN                 NaN                 NaN  ...   \n",
       "4                      NaN                 NaN                 NaN  ...   \n",
       "...                    ...                 ...                 ...  ...   \n",
       "294355                 NaN                 NaN                 NaN  ...   \n",
       "294356                 NaN                 NaN                 NaN  ...   \n",
       "294357                 NaN                 NaN                 NaN  ...   \n",
       "294358                 NaN                 NaN                 NaN  ...   \n",
       "294359                 NaN                 NaN                 NaN  ...   \n",
       "\n",
       "        pixel_value_band_417  pixel_value_band_418  pixel_value_band_419  \\\n",
       "0                        NaN                   NaN                   NaN   \n",
       "1                        NaN                   NaN                   NaN   \n",
       "2                        NaN                   NaN                   NaN   \n",
       "3                        NaN                   NaN                   NaN   \n",
       "4                        NaN                   NaN                   NaN   \n",
       "...                      ...                   ...                   ...   \n",
       "294355                   NaN                   NaN                   NaN   \n",
       "294356                   NaN                   NaN                   NaN   \n",
       "294357                   NaN                   NaN                   NaN   \n",
       "294358                   NaN                   NaN                   NaN   \n",
       "294359                   NaN                   NaN                   NaN   \n",
       "\n",
       "        pixel_value_band_420  pixel_value_band_421  pixel_value_band_422  \\\n",
       "0                        NaN                   NaN                   NaN   \n",
       "1                        NaN                   NaN                   NaN   \n",
       "2                        NaN                   NaN                   NaN   \n",
       "3                        NaN                   NaN                   NaN   \n",
       "4                        NaN                   NaN                   NaN   \n",
       "...                      ...                   ...                   ...   \n",
       "294355                   NaN                   NaN                   NaN   \n",
       "294356                   NaN                   NaN                   NaN   \n",
       "294357                   NaN                   NaN                   NaN   \n",
       "294358                   NaN                   NaN                   NaN   \n",
       "294359                   NaN                   NaN                   NaN   \n",
       "\n",
       "        pixel_value_band_423  pixel_value_band_424  pixel_value_band_425  \\\n",
       "0                        NaN                   NaN                   NaN   \n",
       "1                        NaN                   NaN                   NaN   \n",
       "2                        NaN                   NaN                   NaN   \n",
       "3                        NaN                   NaN                   NaN   \n",
       "4                        NaN                   NaN                   NaN   \n",
       "...                      ...                   ...                   ...   \n",
       "294355                   NaN                   NaN                 823.0   \n",
       "294356                   NaN                   NaN                 166.0   \n",
       "294357                   NaN                   NaN                 339.0   \n",
       "294358                   NaN                   NaN                 491.0   \n",
       "294359                   NaN                   NaN                1211.0   \n",
       "\n",
       "        pixel_value_band_426  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "...                      ...  \n",
       "294355                   NaN  \n",
       "294356                   NaN  \n",
       "294357                   NaN  \n",
       "294358                   NaN  \n",
       "294359                   NaN  \n",
       "\n",
       "[293675 rows x 429 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_columns = [f'pixel_value_band_{i}' for i in range(1, 426)]\n",
    "df_polygons = df_polygons.replace(-9999.0, np.nan)\n",
    "# Drop rows where all values in the specified columns are NaN\n",
    "df_cleaned_polygons = df_polygons.dropna(subset=layer_columns, how='all')\n",
    "df_cleaned_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b7181-c6ec-4197-9bc5-45240c666e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def subset_and_merge_large_csv(large_csv_path, right_df, subset_columns, chunksize=10000):\n",
    "    \"\"\"\n",
    "    Subsets a large CSV by rows/cols combinations found in the smaller dataframe (right_df)\n",
    "    and then merges them, without loading the entire CSV into memory.\n",
    "\n",
    "    Parameters:\n",
    "    - large_csv_path (str): The file path to the large CSV.\n",
    "    - right_df (pd.DataFrame): The smaller dataframe to merge with.\n",
    "    - subset_columns (list of str): The column names to subset and merge on.\n",
    "    - chunksize (int): The number of rows per chunk to read from the CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The resulting merged dataframe.\n",
    "    \"\"\"\n",
    "    # Find unique combinations in the right dataframe\n",
    "    unique_combinations = right_df[subset_columns].drop_duplicates()\n",
    "\n",
    "    # Initialize an empty list to hold chunks\n",
    "    chunk_list = []\n",
    "\n",
    "    # Read the large CSV in chunks\n",
    "    for chunk in pd.read_csv(large_csv_path, chunksize=chunksize):\n",
    "        # Subset the chunk\n",
    "        chunk_subset = chunk.merge(unique_combinations, on=subset_columns, how='inner')\n",
    "        # If the chunk is not empty after subsetting, append it to the list\n",
    "        if not chunk_subset.empty:\n",
    "            chunk_list.append(chunk_subset)\n",
    "\n",
    "    # Concatenate all the relevant chunks\n",
    "    left_df_subset = pd.concat(chunk_list)\n",
    "\n",
    "    # Perform the merge\n",
    "    merged_df = left_df_subset.merge(right_df, on=subset_columns, how='left', suffixes=('', '_right'))\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Usage example\n",
    "# Provide the path to your large CSV and your smaller dataframe\n",
    "large_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectanc_active_pixels.csv'\n",
    "right_df = df_polygons\n",
    "subset_columns = ['Pixel_Row', 'Pixel_Col']\n",
    "merged_df = subset_and_merge_large_csv(large_csv_path, right_df, subset_columns)\n",
    "merged_df\n",
    "# You may want to write the merged dataframe to a new CSV\n",
    "# merged_df.to_csv('/path/to/your/merged.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macrosystems",
   "language": "python",
   "name": "macrosystems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
